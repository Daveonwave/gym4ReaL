{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3837f38",
   "metadata": {},
   "source": [
    "### Step 0 : Init RoboFeeder Env\n",
    "This cell sets up the environment for the RoboFeeder simulation by importing necessary modules and configuring the Python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a674a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "dir = os.getcwd()\n",
    "if 'examples' in dir:\n",
    "    os.chdir(os.getcwd().split('examples')[0])\n",
    "else:\n",
    "    print(\"please set the working directory to the root of the gym4ReaL repository\")\n",
    "\n",
    "# check if the current working directory is the root of the gym4ReaL repository\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8828f27",
   "metadata": {},
   "source": [
    "### SKRL Library\n",
    "install the SKRL library to run Reinforcement learning training procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNCOMMENT TO INSTALL SKRL LIBRARY\n",
    "#!pip install skrl[\"torch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8281830b",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Import Required Libraries and Modules\n",
    "\n",
    "This cell imports all necessary libraries and modules for the RoboFeeder RL environment and training pipeline, including:\n",
    "- Python system and path utilities\n",
    "- The RoboFeeder environment from `gym4real`\n",
    "- PyTorch and neural network modules\n",
    "- SKRL components for RL agent, environment wrappers, memory, models, preprocessors, schedulers, trainers, and utilities\n",
    "- Gymnasium for environment management\n",
    "\n",
    "These imports are essential for building, training, and evaluating the reinforcement learning agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b284b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.getcwd())  # <-- path to the *parent* of gym4real\n",
    "\n",
    "from gym4real.envs.robofeeder.rf_picking_v0 import robotEnv\n",
    "import torch\n",
    "from torch import nn\n",
    "# import the skrl components to build the RL system\n",
    "from skrl.agents.torch.ppo import PPO, PPO_DEFAULT_CONFIG\n",
    "from skrl.envs.loaders.torch import load_isaacgym_env_preview4\n",
    "from skrl.envs.wrappers.torch import wrap_env\n",
    "from skrl.memories.torch import RandomMemory\n",
    "from skrl.models.torch import DeterministicMixin, GaussianMixin, Model\n",
    "from skrl.resources.preprocessors.torch import RunningStandardScaler\n",
    "from skrl.resources.schedulers.torch import KLAdaptiveRL\n",
    "from skrl.trainers.torch import SequentialTrainer\n",
    "from skrl.utils import set_seed\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001be56d",
   "metadata": {},
   "source": [
    "### Step 2: Define the RL Agent and Training Pipeline\n",
    "\n",
    "This cell performs the following key steps:\n",
    "- Loads and wraps the RoboFeeder Isaac Gym environment using a specified configuration file.\n",
    "- Sets up the device for computation (CPU or GPU).\n",
    "- Instantiates a rollout buffer (`RandomMemory`) for storing experiences.\n",
    "- Defines the agent's models, using a shared model for both policy and value functions.\n",
    "- Configures the PPO agent with various hyperparameters, including learning rate, discount factor, and preprocessors for states and values.\n",
    "- Sets up logging and checkpointing for experiment tracking.\n",
    "- Instantiates the PPO agent with the configured models, memory, and environment.\n",
    "- Configures and creates a sequential trainer to manage the training loop.\n",
    "- Starts the training process for the RL agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2bda4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[skrl:INFO] Seed: 50208689\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# seed for reproducibility\n",
    "set_seed()  # e.g. `set_seed(42)` for fixed seed\n",
    "\n",
    "\n",
    "# define shared model (stochastic and deterministic models) using mixins\n",
    "class Shared(GaussianMixin, DeterministicMixin, Model):\n",
    "    def __init__(self, observation_space, action_space, device, clip_actions=False,\n",
    "                 clip_log_std=True, min_log_std=-20, max_log_std=2, reduction=\"sum\"):\n",
    "        Model.__init__(self, observation_space, action_space, device)\n",
    "        GaussianMixin.__init__(self, clip_actions, clip_log_std, min_log_std, max_log_std, reduction)\n",
    "        DeterministicMixin.__init__(self, clip_actions)\n",
    "\n",
    "        self.net = nn.Sequential(nn.Linear(self.num_observations, 400),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(400, 200),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(200, 100),\n",
    "                                 nn.ELU())\n",
    "\n",
    "        self.mean_layer = nn.Linear(100, self.num_actions)\n",
    "        self.log_std_parameter = nn.Parameter(torch.zeros(self.num_actions))\n",
    "\n",
    "        self.value_layer = nn.Linear(100, 1)\n",
    "\n",
    "    def act(self, inputs, role):\n",
    "        if role == \"policy\":\n",
    "            return GaussianMixin.act(self, inputs, role)\n",
    "        elif role == \"value\":\n",
    "            return DeterministicMixin.act(self, inputs, role)\n",
    "\n",
    "    def compute(self, inputs, role):\n",
    "        if role == \"policy\":\n",
    "            self._shared_output = self.net(inputs[\"states\"])\n",
    "            return self.mean_layer(self._shared_output), self.log_std_parameter, {}\n",
    "        elif role == \"value\":\n",
    "            shared_output = self.net(inputs[\"states\"]) if self._shared_output is None else self._shared_output\n",
    "            self._shared_output = None\n",
    "            return self.value_layer(shared_output), {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0e935",
   "metadata": {},
   "source": [
    "### Step 3 : Test Simulator\n",
    "import the simulator configuration file to pass the required parameters to run. \n",
    "Relevant parameters to adapt:\n",
    "\n",
    "#### ObjectToPick\n",
    "    NUMBER_OF_OBJECTS: 1           # (int) Number of objects to pick\n",
    "    SHUFFLE_OBJECTS: True          # (bool) Shuffle object positions at reset\n",
    "    OBJ_CORRECT_ORIENTATION: True  # (bool) Ensure objects have correct orientation\n",
    "\n",
    "#### Simulator Setting\n",
    "    IS_SIMULATION_REAL_TIME: False   # (bool) Run simulation in real time\n",
    "    IS_SIMULATION_SHOWED: True       # (bool) Show simulation window\n",
    "    IS_SIMULATION_RECORD: False      # (bool) Record simulation video\n",
    "    RECORD_FOLDER : \".\" # (str) Folder to save recorded videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Copy the default configuration file to a new editable file\n",
    "default_config_file = os.getcwd() + \"/gym4real/envs/robofeeder/configuration.yaml\"\n",
    "config_file = os.getcwd() + \"/examples/robofeeder/notebooks/configuration_editable.yaml\"\n",
    "shutil.copy(default_config_file, config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cef6a3",
   "metadata": {},
   "source": [
    "### Step 4: Environment Setup, Agent Initialization, and Training\n",
    "\n",
    "This cell performs the following actions:\n",
    "- Wraps the RoboFeeder environment using the edited configuration file and sets the computation device.\n",
    "- Instantiates a rollout buffer (`RandomMemory`) for storing agent experiences.\n",
    "- Defines and shares a neural network model for both policy and value functions.\n",
    "- Configures the PPO agent with hyperparameters such as learning rate, discount factor, and preprocessors for states and values.\n",
    "- Sets up logging and checkpointing for experiment tracking.\n",
    "- Instantiates the PPO agent with the configured models, memory, and environment.\n",
    "- Configures and creates a sequential trainer to manage the training loop.\n",
    "- Starts the training process for the RL agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482dfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/gymnasium/envs/registration.py:519: DeprecationWarning: \u001b[33mWARN: The environment gym4real/robofeeder-picking-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/usr/local/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:118: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "\u001b[38;20m[skrl:INFO] Environment wrapper: 'auto' (class: gymnasium.vector.vector_env.VectorEnv)\u001b[0m\n",
      "\u001b[38;20m[skrl:INFO] Environment wrapper: gymnasium\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.95it/s]\n"
     ]
    }
   ],
   "source": [
    "env = wrap_env(gym.make_vec(\"gym4real/robofeeder-picking-v0\", num_envs=2, config_file= config_file))\n",
    "device = env.device\n",
    "\n",
    "# instantiate a memory as rollout buffer (any memory can be used for this)\n",
    "memory = RandomMemory(memory_size=32, num_envs=env.num_envs, device=device)\n",
    "\n",
    "\n",
    "# instantiate the agent's models (function approximators).\n",
    "# PPO requires 2 models, visit its documentation for more details\n",
    "# https://skrl.readthedocs.io/en/latest/api/agents/ppo.html#models\n",
    "models = {}\n",
    "models[\"policy\"] = Shared(env.observation_space, env.action_space, device)\n",
    "models[\"value\"] = models[\"policy\"]  # same instance: shared model\n",
    "\n",
    "\n",
    "# configure and instantiate the agent (visit its documentation to see all the options)\n",
    "# https://skrl.readthedocs.io/en/latest/api/agents/ppo.html#configuration-and-hyperparameters\n",
    "cfg = PPO_DEFAULT_CONFIG.copy()\n",
    "cfg[\"rollouts\"] = 32  # memory_size\n",
    "cfg[\"learning_epochs\"] = 5\n",
    "cfg[\"mini_batches\"] = 4  # 32 * 4096 / 32768\n",
    "cfg[\"discount_factor\"] = 0.99\n",
    "cfg[\"lambda\"] = 0.95\n",
    "cfg[\"learning_rate\"] = 5e-4\n",
    "cfg[\"learning_rate_scheduler\"] = KLAdaptiveRL\n",
    "cfg[\"learning_rate_scheduler_kwargs\"] = {\"kl_threshold\": 0.008}\n",
    "cfg[\"random_timesteps\"] = 0\n",
    "cfg[\"learning_starts\"] = 0\n",
    "cfg[\"grad_norm_clip\"] = 1.0\n",
    "cfg[\"ratio_clip\"] = 0.2\n",
    "cfg[\"value_clip\"] = 0.2\n",
    "cfg[\"clip_predicted_values\"] = True\n",
    "cfg[\"entropy_loss_scale\"] = 0.0\n",
    "cfg[\"value_loss_scale\"] = 2.0\n",
    "cfg[\"kl_threshold\"] = 0\n",
    "cfg[\"rewards_shaper\"] = lambda rewards, timestep, timesteps: rewards * 0.01\n",
    "cfg[\"state_preprocessor\"] = RunningStandardScaler\n",
    "cfg[\"state_preprocessor_kwargs\"] = {\"size\": env.observation_space, \"device\": device}\n",
    "cfg[\"value_preprocessor\"] = RunningStandardScaler\n",
    "cfg[\"value_preprocessor_kwargs\"] = {\"size\": 1, \"device\": device}\n",
    "# logging to TensorBoard and write checkpoints (in timesteps)\n",
    "cfg[\"experiment\"][\"write_interval\"] = 160\n",
    "cfg[\"experiment\"][\"checkpoint_interval\"] = 1600\n",
    "cfg[\"experiment\"][\"directory\"] = \"runs/torch/Humanoid\"\n",
    "\n",
    "agent = PPO(models=models,\n",
    "            memory=memory,\n",
    "            cfg=cfg,\n",
    "            observation_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            device=device)\n",
    "\n",
    "\n",
    "# configure and instantiate the RL trainer\n",
    "cfg_trainer = {\"timesteps\": 10, \"headless\": True}\n",
    "trainer = SequentialTrainer(cfg=cfg_trainer, env=env, agents=agent)\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
