{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 : Init RoboFeeder Env\n",
    "This cell sets up the environment for the RoboFeeder simulation by importing necessary modules and configuring the Python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "dir = os.getcwd()\n",
    "if 'gym4ReaL' in dir:\n",
    "    os.chdir(os.getcwd().split('gym4ReaL')[0] + 'gym4ReaL')\n",
    "else:\n",
    "    print(\"please set the working directory to the root of the gym4ReaL repository\")\n",
    "\n",
    "# check if the current working directory is the root of the gym4ReaL repository\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Import Required Modules\n",
    "This cell imports the necessary modules and updates the system path to include the gym4ReaL repository. It also imports the robot simulator and `stable-baseline` libraries to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.getcwd())  # <-- path to the *parent* of gym4real\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from gym4real.envs.robofeeder.rf_picking_v0 import robotEnv\n",
    "\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv,DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Test Simulator\n",
    "import the simulator configuration file to pass the required parameters to run. \n",
    "Relevant parameters to adapt:\n",
    "\n",
    "#### ObjectToPick\n",
    "    NUMBER_OF_OBJECTS: 1           # (int) Number of objects to pick\n",
    "    SHUFFLE_OBJECTS: True          # (bool) Shuffle object positions at reset\n",
    "    OBJ_CORRECT_ORIENTATION: True  # (bool) Ensure objects have correct orientation\n",
    "\n",
    "#### Simulator Setting\n",
    "    IS_SIMULATION_REAL_TIME: False   # (bool) Run simulation in real time\n",
    "    IS_SIMULATION_SHOWED: True       # (bool) Show simulation window\n",
    "    IS_SIMULATION_RECORD: False      # (bool) Record simulation video\n",
    "    RECORD_FOLDER : \".\" # (str) Folder to save recorded videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Copy the default configuration file to a new editable file\n",
    "default_config_file = os.getcwd() + \"/gym4real/envs/robofeeder/configuration.yaml\"\n",
    "config_file = os.getcwd() + \"/examples/robofeeder/notebooks/configuration_editable.yaml\"\n",
    "shutil.copy(default_config_file, config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3 : Define Environment Creation Function\n",
    "\n",
    "This cell defines a helper function `make_env` that creates and returns a new instance of the `robotEnv` environment using the specified configuration file. This function is used to generate multiple environments for parallel training with vectorized environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the environment to stack in a vectorEnv\n",
    "def make_env(config_file):\n",
    "    def _init():\n",
    "        env = robotEnv(config_file=config_file)\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Create Vectorized Environment\n",
    "\n",
    "This cell sets up a vectorized environment using `SubprocVecEnv` to enable parallel simulation of multiple environments. It uses the `make_env` function to create separate instances of the `robotEnv` environment, each with its own process. The number of parallel environments is determined by `num_cpu`. This setup is essential for efficient training of reinforcement learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:118: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:118: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_cpu = 2 # Number of processes/Env to use\n",
    "env = SubprocVecEnv([make_env(config_file) for i in range(1,num_cpu+ 1)]) # Create the vectorEnv (ROS_ID start at 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 5 : Define Custom CNN Feature Extractor and Policy\n",
    "\n",
    "This cell defines a custom convolutional neural network (CNN) feature extractor by subclassing `BaseFeaturesExtractor` from Stable Baselines3. The custom extractor processes image observations for the reinforcement learning agent. It also sets up the policy architecture and optimizer parameters for the PPO agent, specifying the network layers for the policy (`pi`) and value function (`vf`), as well as other relevant hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        ks = 3\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 16, kernel_size=ks, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=ks, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=ks, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=ks, stride=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Dynamically calculate CNN output size\n",
    "        with th.no_grad():\n",
    "            dummy_input = th.zeros(1, *observation_space.shape)\n",
    "            flat_output = self.cnn(dummy_input)\n",
    "            cnn_output_dim = flat_output.shape[1]\n",
    "            #print(f\"Raw CNN output dim: {cnn_output_dim}\")\n",
    "\n",
    "        # Final projection to fixed feature dim\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(cnn_output_dim, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self._features_dim = features_dim\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        features = self.cnn(observations)\n",
    "        return self.linear(features)\n",
    "\n",
    "\n",
    "pi = [256,256,128]\n",
    "vf = [256,256,128]\n",
    "\n",
    "features_dim = 256\n",
    "optimizer_kwargs= dict (weight_decay=1e-5,)\n",
    "\n",
    "\n",
    "policy_kwargs = dict(normalize_images=False,\n",
    "                     features_extractor_class=CustomCNN,\n",
    "                     features_extractor_kwargs=dict(features_dim=features_dim),\n",
    "                     net_arch=dict(pi=pi, vf=vf),\n",
    "                     optimizer_kwargs=optimizer_kwargs\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 : Initialize PPO Model\n",
    "\n",
    "This cell initializes the Proximal Policy Optimization (PPO) model using the custom CNN policy defined earlier. The model is configured with the vectorized environment, custom policy architecture, optimizer parameters, and other relevant hyperparameters such as number of steps, batch size, learning rate, and entropy coefficient. This setup prepares the reinforcement learning agent for training on the RoboFeeder environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps=3\n",
    "\n",
    "model = PPO(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    n_steps=n_steps,\n",
    "    batch_size=n_steps*num_cpu,\n",
    "    n_epochs=20,\n",
    "    learning_rate=0.003, \n",
    "    clip_range=0.3,\n",
    "    #gamma=0.95,\n",
    "    ent_coef=0.01, \n",
    "    #vf_coef=0.5,\n",
    "    #max_grad_norm=.5,\n",
    "    verbose=0,\n",
    "    seed=123,\n",
    "    tensorboard_log= \".\",\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 : Train the PPO Model\n",
    "\n",
    "This cell starts the training process for the PPO model using the vectorized RoboFeeder environment. The `learn` method is called with a specified number of timesteps (`total_timesteps=100`). The `reset_num_timesteps=False` argument ensures that the training continues from the current timestep count, and `progress_bar=True` displays a progress bar during training. This step is essential for teaching the agent to interact with the environment and improve its performance through reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7052cb777910>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100,reset_num_timesteps=False,progress_bar=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
