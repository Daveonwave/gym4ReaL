{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2551241",
   "metadata": {},
   "source": [
    "# Simple PPO agent\n",
    "\n",
    "The notebook provides a quick training of [Proximal-Policy Optimization](https://arxiv.org/abs/1707.06347) (PPO) algorithm on the `MicroGridEnv` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ffdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "import gymnasium as gym\n",
    "\n",
    "from gym4real.envs.microgrid.utils import parameter_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d063ac-c1aa-4a69-bf85-c3944f1eb865",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plot_colors = sns.color_palette()\n",
    "sns.set(font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9efdeb",
   "metadata": {},
   "source": [
    "## PPO Agent\n",
    "We are adopting the Stable-Baselines 3 version of PPO, described [here](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html).\n",
    "\n",
    "Here we initialize both the environment for training the agent and the environment to evaluate the agent. Indeed, the evaluation is done on an environment which presents different consumption profiles. The evaluation is done on 5 profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2961f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install stable-baselines3\n",
    "#!pip install stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de18bcc-5f82-416a-b04f-999d14425305",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 5\n",
    "n_envs = 4\n",
    "\n",
    "# Validation profiles belonging to the train set\n",
    "eval_profiles = [350, 351, 352, 353, 354] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa19f7f-ab98-4c23-8c84-446891c3ca61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = parameter_generator(world_options='gym4real/envs/microgrid/world_train.yaml')\n",
    "env = make_vec_env(\"gym4real/microgrid-v0\", n_envs=n_envs, env_kwargs={'settings':params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8067927f-a337-440b-894b-ed9f4182d986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PPO(MlpPolicy, env)\n",
    "model.learn(total_timesteps=len(env.get_attr('generation')[0]) * n_envs * n_episodes, \n",
    "            progress_bar=True)\n",
    "model.save('examples/microgrid/trained_models/PPO_quick')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214ee1fa-d382-413b-b0b7-877483b5c8fb",
   "metadata": {},
   "source": [
    "## Comparison with Random policy\n",
    "\n",
    "Here we will compare the PPO model saved with a simple random policy. The policies will be compared on several test profiles never seen before by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150be22-d531-4c54-90cb-388620db3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_params = parameter_generator(world_options='gym4real/envs/microgrid/world_test.yaml')\n",
    "\n",
    "# Test profiles belonging to the test set\n",
    "test_profiles = [370, 371, 372, 373, 374, 375, 376, 377, 378, 379]\n",
    "rewards = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c54d0",
   "metadata": {},
   "source": [
    "### Random Policy\n",
    "The action is chosen randomly at each decision step by randomly sampling within the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e6651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"gym4real/microgrid-v0\", **{'settings':eval_params})\n",
    "\n",
    "alg = 'random'\n",
    "rewards[alg] = {}\n",
    "\n",
    "for profile in tqdm(test_profiles):\n",
    "    obs, info = env.reset(options={'eval_profile': str(profile)})\n",
    "    done = False\n",
    "    cumulated_reward = 0\n",
    "    rewards[alg][profile] = []\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Randomly select an action\n",
    "        obs, reward, terminated, truncated, info = env.step(action)  \n",
    "        done = terminated or truncated\n",
    "        cumulated_reward += reward\n",
    "        rewards[alg][profile].append(cumulated_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f35066-0d9a-4e07-a9d1-6e7863acda64",
   "metadata": {},
   "source": [
    "### PPO agent\n",
    "Here we load the previously created model `PPO_quick`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1271c-39b9-4dc9-bfe0-1c19afdabaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(\"gym4real/microgrid-v0\", n_envs=1, env_kwargs={'settings':eval_params})\n",
    "\n",
    "alg = 'ppo'\n",
    "rewards[alg] = {}\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=1)\n",
    "vec_env = model.get_env()\n",
    "model = PPO.load(\"examples/microgrid/trained_models/PPO_quick\")\n",
    "\n",
    "for profile in tqdm(test_profiles):\n",
    "    vec_env.set_options({'eval_profile': str(profile)})\n",
    "    obs = vec_env.reset()\n",
    "\n",
    "    cumulated_reward = 0\n",
    "    rewards[alg][profile] = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, r, dones, info = vec_env.step(action)\n",
    "        done = dones[0]\n",
    "        cumulated_reward += r[0]\n",
    "        rewards[alg][profile].append(cumulated_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40212b46-1e6d-4072-ae55-c0cdd02c6b6b",
   "metadata": {},
   "source": [
    "Let's compare the cumulative rewards averaged among the test profiles between `PPO` undergone a quick training and the `random` policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4), tight_layout=True)\n",
    "\n",
    "for i, alg in enumerate(rewards.keys()):\n",
    "    means = np.mean([(rewards[alg][profile]) for profile in rewards[alg].keys()], axis=0)\n",
    "    stds = np.std([(rewards[alg][profile]) for profile in rewards[alg].keys()], axis=0)\n",
    "    ci = 1.96 * stds/np.sqrt(len(rewards[alg].keys()))\n",
    "    \n",
    "    ax.plot(means, label=alg)        \n",
    "    ax.fill_between(range(len(means)), means + ci, means - ci, alpha=0.1)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf099e49-396c-42f1-94a3-5c2cd420619f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gym4real)",
   "language": "python",
   "name": "gym4real"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
