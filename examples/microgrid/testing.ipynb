{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2551241",
   "metadata": {},
   "source": [
    "# Testing and comparing different strategies\n",
    "\n",
    "The notebook provides testing and comparison between deterministic policies and RL agent strategies among the ErNESTO-gym single agent environment (`MicroGridEnv`). <br>\n",
    "Refer to the `training.ipynb` notebook to learn how to use ErNESTO-gym to train RL models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ffdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "from ernestogym.envs import MicroGridEnv\n",
    "from ernestogym.envs.single_agent.utils import parameter_generator\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "plot_colors = sns.color_palette()\n",
    "sns.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_color = OrderedDict({\n",
    "    'random': plot_colors[0],\n",
    "    'only_market': plot_colors[1],\n",
    "    'battery_first': plot_colors[2],\n",
    "    '20-80': plot_colors[3],\n",
    "    '50-50': plot_colors[4],\n",
    "    'ppo': plot_colors[5],\n",
    "    'a2c': plot_colors[6],\n",
    "    'sac': plot_colors[7]\n",
    "})\n",
    "\n",
    "alg_markers = OrderedDict({\n",
    "    'random': '.',\n",
    "    'only_market': 'o',\n",
    "    'battery_first': 'v',\n",
    "    '20-80': 's',\n",
    "    '50-50': 'P',\n",
    "    'ppo': '*',\n",
    "    'a2c': '+',\n",
    "    'sac': '<'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae561628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(timestamps: list, res: dict, algs: list, reward_type='weighted_reward', sampling_rate=10):\n",
    "    fig, ((ax1), (ax2), (ax3)) = plt.subplots(3, 1, figsize=(12, 12), tight_layout=True)\n",
    "    \n",
    "    for i, alg in enumerate(algs):\n",
    "        trad_list = res[alg][reward_type]['r_trad']   \n",
    "        op_list = res[alg][reward_type]['r_op']     \n",
    "        clip_list = res[alg][reward_type]['r_clip']     \n",
    "        \n",
    "        ax1.plot(timestamps[::sampling_rate], trad_list[::sampling_rate], label=alg, color=alg_color[alg], marker=alg_markers[alg], markevery=5000)\n",
    "        ax1.set(xlabel='Samples', ylabel='R_trad')   \n",
    "\n",
    "        ax2.plot(timestamps[::sampling_rate], op_list[::sampling_rate], label=alg, color=alg_color[alg], marker=alg_markers[alg], markevery=5000)\n",
    "        ax2.set(xlabel='Samples', ylabel='R_op') \n",
    "        \n",
    "        ax3.plot(timestamps[::sampling_rate], clip_list[::sampling_rate], label=alg, color=alg_color[alg], marker=alg_markers[alg], markevery=5000)\n",
    "        ax3.set(xlabel='Samples', ylabel='R_clip')\n",
    "        \n",
    "        ax1.legend()\n",
    "    \n",
    "def plot_cum_rewards(timestamps: list, res: dict, algs: list, sampling_rate=10, reward_type='weighted_reward'):\n",
    "    data = {}\n",
    "    fig, ((ax1), (ax2), (ax3)) = plt.subplots(3, 1, figsize=(12, 12), tight_layout=True)\n",
    "    \n",
    "    for i, alg in enumerate(algs):\n",
    "        means =res[alg][reward_type]['r_trad']\n",
    "        ax1.plot(timestamps[::sampling_rate], np.cumsum(means)[::sampling_rate], label=alg, color=alg_color[alg], marker=alg_markers[alg], markevery=5000)\n",
    "        ax1.set(xlabel='Samples', ylabel='R_trad')   \n",
    "        \n",
    "        means = res[alg][reward_type]['r_op']\n",
    "        ax2.plot(timestamps[::sampling_rate], np.cumsum(means)[::sampling_rate], label=alg, color=alg_color[alg], marker=alg_markers[alg], markevery=5000)\n",
    "        ax2.set(xlabel='Samples', ylabel='R_op') \n",
    "        \n",
    "        means = res[alg][reward_type]['r_clip']\n",
    "        ax3.plot(timestamps[::sampling_rate], np.cumsum(means)[::sampling_rate], label=alg, color=alg_color[alg], marker=alg_markers[alg], markevery=5000)\n",
    "        ax3.set(xlabel='Samples', ylabel='R_clip')\n",
    "        \n",
    "        ax1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9efdeb",
   "metadata": {},
   "source": [
    "## Environment Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da659048",
   "metadata": {},
   "outputs": [],
   "source": [
    "pack_options = \"ernestogym/ernesto/data/battery/pack.yaml\"\n",
    "# ecm = \"ernestogym/ernesto/data/battery/models/electrical/thevenin_pack.yaml\"\n",
    "ecm = \"ernestogym/ernesto/data/battery/models/electrical/thevenin_fading_pack.yaml\"\n",
    "r2c = \"ernestogym/ernesto/data/battery/models/thermal/r2c_thermal_pack.yaml\"\n",
    "bolun = \"ernestogym/ernesto/data/battery/models/aging/bolun_pack.yaml\"\n",
    "# world = \"ernestogym/envs/single_agent/world_deg.yaml\"\n",
    "world = \"ernestogym/envs/single_agent/world_fading.yaml\"\n",
    "\n",
    "params = parameter_generator(\n",
    "    battery_options=pack_options,\n",
    "    electrical_model=ecm,\n",
    "    thermal_model=r2c,\n",
    "    aging_model=bolun,\n",
    "    world_options=world,\n",
    "    use_reward_normalization=True\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292e747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = MicroGridEnv(settings=params)\n",
    "\n",
    "print('Size of State Space: ', env.observation_space.shape)\n",
    "print('Observation Space: ', env.spaces.keys())\n",
    "print('Size of Action Space: ', env.action_space.shape)\n",
    "print('Min action: ', env.action_space.low)\n",
    "print('Max action: ', env.action_space.high)\n",
    "print('Sample State: ', env.observation_space.sample())\n",
    "print('Sample Action: ', env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84953099",
   "metadata": {},
   "source": [
    "## Experiment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b1615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_steps = len(env.demand)\n",
    "num_steps = 100000\n",
    "timestamps = env.demand.timestamps[:num_steps]\n",
    "timestamps = [datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\") for time in timestamps]\n",
    "\n",
    "comparison_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733076f5",
   "metadata": {},
   "source": [
    "---\n",
    "## Deterministic policies\n",
    "Here we can evaluate different rule-based and deterministic policies.\n",
    "Hereafter we will test:\n",
    "1. random action policy\n",
    "2. market-only policy\n",
    "3. battery-first policy\n",
    "4. 20/80 policy\n",
    "5. 50/50 policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c54d0",
   "metadata": {},
   "source": [
    "<h3> Random Policy: </h3>\n",
    "The action is chosen randomly at each decision step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e6651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alg = 'random'\n",
    "env.reset(options={'eval_profile': '67'})\n",
    "\n",
    "for i in tqdm(range(num_steps)):\n",
    "    action = env.action_space.sample()  # Randomly select an action\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)  # Return observation and reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_dict[alg] = {}\n",
    "comparison_dict[alg]['total_reward'] = env.total_reward\n",
    "comparison_dict[alg]['pure_reward'] = env.pure_reward_list\n",
    "comparison_dict[alg]['norm_reward'] = env.norm_reward_list\n",
    "comparison_dict[alg]['weighted_reward'] = env.weighted_reward_list\n",
    "comparison_dict[alg]['actions'] = [action.tolist() for action in env.action_list]\n",
    "comparison_dict[alg]['states'] = [state.tolist() for state in env.state_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebbc2c",
   "metadata": {},
   "source": [
    "<h3> Only Market Policy: </h3>\n",
    "The action chosen is always 0, meaning that the battery is never used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab672d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = 'only_market'\n",
    "env.reset(options={'eval_profile': '67'})\n",
    "\n",
    "for i in tqdm(range(num_steps)):\n",
    "    action = [0.]  # Only market\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)  # Return observation and reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d68214",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_dict[alg] = {}\n",
    "comparison_dict[alg]['total_reward'] = env.total_reward\n",
    "comparison_dict[alg]['pure_reward'] = env.pure_reward_list\n",
    "comparison_dict[alg]['norm_reward'] = env.norm_reward_list\n",
    "comparison_dict[alg]['weighted_reward'] = env.weighted_reward_list\n",
    "comparison_dict[alg]['actions'] = [action for action in env.action_list]\n",
    "comparison_dict[alg]['states'] = [state for state in env.state_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f24d08",
   "metadata": {},
   "source": [
    "<h3> Battery First Policy: </h3>\n",
    "The action chosen is always 1, meaning that the battery is always used before interacting with the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = 'battery_first'\n",
    "env.reset(options={'eval_profile': '67'})\n",
    "\n",
    "for i in tqdm(range(num_steps)):\n",
    "    action = [1.]  # Only market\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)  # Return observation and reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_dict[alg] = {}\n",
    "comparison_dict[alg]['total_reward'] = env.total_reward\n",
    "comparison_dict[alg]['pure_reward'] = env.pure_reward_list\n",
    "comparison_dict[alg]['norm_reward'] = env.norm_reward_list\n",
    "comparison_dict[alg]['weighted_reward'] = env.weighted_reward_list\n",
    "comparison_dict[alg]['actions'] = [action for action in env.action_list]\n",
    "comparison_dict[alg]['states'] = [state for state in env.state_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ccdec",
   "metadata": {},
   "source": [
    "<h3> 20-80 Policy: </h3>\n",
    "The action chosen is always 0.2, meaning that the battery is used only for the 20% of the margin energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1575ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = '20-80'\n",
    "env.reset(options={'eval_profile': '67'})\n",
    "\n",
    "for i in tqdm(range(num_steps)):\n",
    "    action = [0.2]  # Only market\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)  # Return observation and reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_dict[alg] = {}\n",
    "comparison_dict[alg]['total_reward'] = env.total_reward\n",
    "comparison_dict[alg]['pure_reward'] = env.pure_reward_list\n",
    "comparison_dict[alg]['norm_reward'] = env.norm_reward_list\n",
    "comparison_dict[alg]['weighted_reward'] = env.weighted_reward_list\n",
    "comparison_dict[alg]['actions'] = [action for action in env.action_list]\n",
    "comparison_dict[alg]['states'] = [state for state in env.state_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4cf9a6",
   "metadata": {},
   "source": [
    "<h3> 50-50 Policy: </h3>\n",
    "The action chosen is always 0.5, meaning that the battery is never used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670df112",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = '50-50'\n",
    "env.reset(options={'eval_profile': '67'})\n",
    "\n",
    "for i in tqdm(range(num_steps)):\n",
    "    action = [0.5]  # Only market\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)  # Return observation and reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_dict[alg] = {}\n",
    "comparison_dict[alg]['total_reward'] = env.total_reward\n",
    "comparison_dict[alg]['pure_reward'] = env.pure_reward_list\n",
    "comparison_dict[alg]['norm_reward'] = env.norm_reward_list\n",
    "comparison_dict[alg]['weighted_reward'] = env.weighted_reward_list\n",
    "comparison_dict[alg]['actions'] = [action for action in env.action_list]\n",
    "comparison_dict[alg]['states'] = [state for state in env.state_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed9c59",
   "metadata": {},
   "source": [
    "---\n",
    "## Reinforcement Learning Agents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab00ac",
   "metadata": {},
   "source": [
    "<h3>PPO Agent</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170df98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"examples/single_agent/models/ppo_100000_steps.zip\", env=env)\n",
    "    \n",
    "alg = 'ppo'    \n",
    "actions = [] \n",
    "vec_env = model.get_env()\n",
    "vec_env.set_options({'eval_profile': '67'})\n",
    "obs = vec_env.reset()\n",
    "for i in tqdm(range(num_steps)):\n",
    "    action, _states = model.predict(obs)\n",
    "    actions.append(action)\n",
    "    obs, rewards, dones, info = vec_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c9883",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_dict[alg] = {}\n",
    "comparison_dict[alg]['total_reward'] = env.total_reward\n",
    "comparison_dict[alg]['pure_reward'] = env.pure_reward_list\n",
    "comparison_dict[alg]['norm_reward'] = env.norm_reward_list\n",
    "comparison_dict[alg]['weighted_reward'] = env.weighted_reward_list\n",
    "comparison_dict[alg]['actions'] = [action for action in env.action_list]\n",
    "comparison_dict[alg]['states'] = [state for state in env.state_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6818aa2",
   "metadata": {},
   "source": [
    "<h3>A2C Agent</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d387cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"examples/single_agent/models/a2c_100000_steps.zip\", env=env)\n",
    "    \n",
    "alg = 'a2c'    \n",
    "actions = [] \n",
    "vec_env = model.get_env()\n",
    "vec_env.set_options({'eval_profile': '67'})\n",
    "obs = vec_env.reset()\n",
    "for i in tqdm(range(num_steps)):\n",
    "    action, _states = model.predict(obs)\n",
    "    actions.append(action)\n",
    "    obs, rewards, dones, info = vec_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f5d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_dict[alg] = {}\n",
    "comparison_dict[alg]['total_reward'] = env.total_reward\n",
    "comparison_dict[alg]['pure_reward'] = env.pure_reward_list\n",
    "comparison_dict[alg]['norm_reward'] = env.norm_reward_list\n",
    "comparison_dict[alg]['weighted_reward'] = env.weighted_reward_list\n",
    "comparison_dict[alg]['actions'] = [action for action in env.action_list]\n",
    "comparison_dict[alg]['states'] = [state for state in env.state_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e0a44",
   "metadata": {},
   "source": [
    "## Comparison between the different baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa9003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "algs_to_compare = ['random', 'only_market', 'battery_first', '20-80', '50-50', 'ppo', 'a2c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1531f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(timestamps=timestamps, res=comparison_dict, algs=algs_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cum_rewards(timestamps=timestamps, res=comparison_dict, algs=algs_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeacd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ernesto-gym",
   "language": "python",
   "name": "ernesto-gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
